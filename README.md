# Sistema RAG (Retrieval-Augmented Generation) com pgVector

Sistema completo de RAG que realiza a ingestÃ£o de documentos PDF, converte o conteÃºdo em embeddings, armazena no PostgreSQL com pgVector e disponibiliza um chat interativo via CLI para consultas semÃ¢nticas.

## Requisitos

- Python 3.8+
- PostgreSQL com extensÃ£o pgVector instalada
- Conta OpenAI com API Key ativa **OU** Google AI API Key ativa

## InstalaÃ§Ã£o

### 1. Configurar ambiente virtual Python

Crie e ative um ambiente virtual antes de instalar as dependÃªncias:

**Linux/macOS:**
```bash
python3 -m venv venv
source venv/bin/activate
```

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

### 2. Instalar dependÃªncias Python

Com o ambiente virtual ativado, instale as dependÃªncias:

```bash
pip install python-dotenv langchain-community langchain-text-splitters langchain-openai langchain-google-genai langchain-core langchain-postgres pypdf psycopg
```

Ou instale com:

```bash
pip install -r requirements.txt
```

### 3. Configurar PostgreSQL com pgVector

Certifique-se de que o PostgreSQL estÃ¡ rodando e a extensÃ£o pgVector estÃ¡ instalada.

#### Usando Docker Compose (Recomendado):

Suba o banco de dados:

```bash
docker compose up -d
```

#### Ou usando Docker (comando direto):

```bash
docker run -d \
  --name postgres-pgvector \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_DB=rag \
  -p 5432:5432 \
  pgvector/pgvector:pg16
```

#### Ou instalando manualmente:

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

## ConfiguraÃ§Ã£o

### 1. Criar arquivo `.env`

Crie um arquivo `.env` na raiz do projeto com as seguintes variÃ¡veis:

```dotenv
# Provider Configuration
EMBEDDING_PROVIDER=openai  # OpÃ§Ãµes: "openai" ou "google"
LLM_PROVIDER=openai        # OpÃ§Ãµes: "openai" ou "google"

# Google AI Configuration
GOOGLE_API_KEY=sua-chave-google-aqui
GOOGLE_EMBEDDING_MODEL=models/embedding-001
GOOGLE_LLM_MODEL=gemini-2.5-flash-lite

# OpenAI Configuration
OPENAI_API_KEY=sua-chave-openai-aqui
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_LLM_MODEL=gpt-5-nano

# Database Configuration
DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/rag

# pgVector Configuration
PG_VECTOR_COLLECTION_NAME=rag

# PDF Configuration
PDF_PATH=document.pdf
```

**Nota sobre Providers:**
- `EMBEDDING_PROVIDER`: Define qual provedor usar para gerar embeddings (vetorizaÃ§Ã£o)
- `LLM_PROVIDER`: Define qual LLM usar para gerar respostas no chat
- VocÃª pode usar OpenAI para embeddings e Google para LLM (ou vice-versa)
- Preencha apenas as API Keys dos providers que for utilizar

### 2. Adicionar seu documento PDF

Coloque o arquivo PDF que deseja processar no diretÃ³rio do projeto e atualize a variÃ¡vel `PDF_PATH` no arquivo `.env` com o caminho correto.

## Estrutura do Projeto

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py          # Script de ingestÃ£o de PDFs
â”‚   â”œâ”€â”€ search.py          # MÃ³dulo de busca vetorial e LLM
â”‚   â””â”€â”€ chat.py            # Interface CLI do chat interativo
â”œâ”€â”€ docker-compose.yml     # ConfiguraÃ§Ã£o do PostgreSQL
â”œâ”€â”€ .env                   # VariÃ¡veis de ambiente (nÃ£o commitar!)
â”œâ”€â”€ document.pdf           # Seu documento PDF
â”œâ”€â”€ requirements.txt       # DependÃªncias Python
â”œâ”€â”€ venv/                  # Ambiente virtual Python (nÃ£o commitar!)
â””â”€â”€ README.md             # Este arquivo
```

## ExecuÃ§Ã£o

### Ordem de ExecuÃ§Ã£o

Siga estes passos na ordem correta:

#### 1. Subir o banco de dados

```bash
docker compose up -d
```

Aguarde alguns segundos para o PostgreSQL inicializar completamente.

#### 2. Executar a ingestÃ£o do PDF

Processe e armazene o documento no banco vetorial:

```bash
python src/ingest.py
```

**SaÃ­da esperada:**

```
Loading PDF: document.pdf
Splitting documents into chunks...
Created 45 chunks
Creating embeddings...
Using OpenAI embeddings: text-embedding-3-small
Connecting to PostgreSQL with pgVector...
Storing documents in database...
Successfully ingested 45 chunks into the database
```

#### 3. Iniciar o chat interativo

ApÃ³s a ingestÃ£o bem-sucedida, inicie o chat para fazer perguntas:

```bash
python src/chat.py
```

**Interface do chat:**

```
============================================================
ğŸ¤– Chat RAG - Sistema de Consulta
============================================================

ğŸ“Š Embeddings: OPENAI
ğŸ§  LLM: OPENAI

ğŸ’¡ Comandos disponÃ­veis:
  - Digite sua pergunta para buscar no documento
  - Digite 'sources' para ver as fontes da Ãºltima resposta
  - Digite 'clear' para limpar a tela
  - Digite 'sair' ou 'exit' para encerrar
============================================================

âœ… Sistema inicializado com sucesso!

ğŸ§‘ VocÃª: Qual Ã© o tema principal do documento?
ğŸ¤– Assistente: [Resposta baseada no conteÃºdo do PDF]

ğŸ§‘ VocÃª: sources
ğŸ” Buscando fontes...
============================================================
ğŸ“š FONTES CONSULTADAS
============================================================
[Mostra os 10 trechos mais relevantes consultados]
```

## Como Funciona

### IngestÃ£o (ingest.py)

1. **Carregamento do PDF**: O documento Ã© lido usando `PyPDFLoader`
2. **DivisÃ£o em chunks**: O texto Ã© dividido em pedaÃ§os de 1000 caracteres com overlap de 150 caracteres
3. **GeraÃ§Ã£o de embeddings**: Cada chunk Ã© convertido em vetor usando o modelo configurado (OpenAI ou Google)
4. **Armazenamento**: Os vetores sÃ£o salvos no PostgreSQL com pgVector para busca semÃ¢ntica

### Consulta (search.py + chat.py)

1. **VetorizaÃ§Ã£o da pergunta**: A pergunta do usuÃ¡rio Ã© convertida em embedding
2. **Busca semÃ¢ntica**: Busca os 10 chunks mais relevantes (k=10) no banco vetorial usando similaridade de cosseno
3. **Montagem do prompt**: Monta um prompt estruturado com o contexto recuperado e a pergunta
4. **Chamada Ã  LLM**: Envia o prompt para a LLM configurada (OpenAI ou Google)
5. **Resposta**: Retorna a resposta gerada baseada apenas no contexto fornecido

### Regras do Sistema

O sistema segue regras rigorosas para evitar alucinaÃ§Ãµes:
- âœ… Responde **apenas** com base no contexto recuperado
- âŒ **Nunca** inventa informaÃ§Ãµes
- âŒ **Nunca** usa conhecimento externo ao documento
- â„¹ï¸ Se a informaÃ§Ã£o nÃ£o estiver no contexto, informa explicitamente ao usuÃ¡rio

## ParÃ¢metros ConfigurÃ¡veis

### Chunking (ingest.py)
- **chunk_size**: 1000 caracteres
- **chunk_overlap**: 150 caracteres
- **add_start_index**: False

### Busca Vetorial (search.py)
- **k**: 10 documentos mais relevantes
- **search_type**: similarity (busca por similaridade de cosseno)

### LLM
- **temperature**: 0 (respostas determinÃ­sticas)

Esses parÃ¢metros podem ser ajustados diretamente nos arquivos conforme necessÃ¡rio.

## SoluÃ§Ã£o de Problemas

### Erro: "Environment variable X is not set"

Verifique se todas as variÃ¡veis necessÃ¡rias estÃ£o configuradas no arquivo `.env`.

### Erro: "PDF file not found"

Certifique-se de que o caminho do PDF no `.env` estÃ¡ correto e o arquivo existe.

### Erro de conexÃ£o com PostgreSQL

- Verifique se o PostgreSQL estÃ¡ rodando
- Confirme as credenciais e porta no `DATABASE_URL`
- Se usar Docker, use `host.docker.internal` ao invÃ©s de `localhost` quando o script roda dentro de um container

### Erro com API OpenAI

- Verifique se sua chave API estÃ¡ ativa
- Confirme se hÃ¡ crÃ©ditos disponÃ­veis na conta OpenAI
- Verifique se o modelo estÃ¡ correto (`gpt-5-nano` para LLM, `text-embedding-3-small` para embeddings)

### Erro com API Google

- Verifique se sua chave API estÃ¡ ativa no Google AI Studio
- Confirme se os modelos estÃ£o corretos (`gemini-2.5-flash-lite` para LLM, `models/embedding-001` para embeddings)

### Chat nÃ£o encontra respostas no documento

- Verifique se a ingestÃ£o foi concluÃ­da com sucesso
- Confirme que o `EMBEDDING_PROVIDER` Ã© o mesmo na ingestÃ£o e no chat
- Tente reformular a pergunta de forma mais especÃ­fica
- Use o comando `sources` para ver quais trechos foram consultados

## Funcionalidades

### IngestÃ£o de Documentos
- âœ… Suporte para arquivos PDF
- âœ… Chunking inteligente com overlap
- âœ… Suporte para embeddings OpenAI e Google
- âœ… Armazenamento vetorial com pgVector
- âœ… PreservaÃ§Ã£o de metadados do documento

### Chat Interativo
- âœ… Interface CLI amigÃ¡vel
- âœ… Busca semÃ¢ntica com 10 resultados mais relevantes
- âœ… Respostas baseadas apenas no contexto do documento
- âœ… Comando para visualizar fontes consultadas
- âœ… Suporte para mÃºltiplos providers de LLM
- âœ… PrevenÃ§Ã£o de alucinaÃ§Ãµes com prompt estruturado
- âœ… Comandos Ãºteis (sources, clear, exit)

## Comandos do Chat

| Comando | DescriÃ§Ã£o |
|---------|-----------|
| `<sua pergunta>` | Faz uma pergunta sobre o documento |
| `sources` | Mostra as fontes da Ãºltima resposta |
| `clear` | Limpa a tela do terminal |
| `sair` / `exit` | Encerra o chat |

## Exemplos de Uso

### Exemplo 1: Pergunta dentro do contexto

```
ğŸ§‘ VocÃª: Quais sÃ£o os principais tÃ³picos abordados no documento?
ğŸ¤– Assistente: Com base no documento, os principais tÃ³picos sÃ£o...
```

### Exemplo 2: Pergunta fora do contexto

```
ğŸ§‘ VocÃª: Qual Ã© a capital da FranÃ§a?
ğŸ¤– Assistente: NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta.
```

### Exemplo 3: Consultando fontes

```
ğŸ§‘ VocÃª: sources
ğŸ” Buscando fontes...
============================================================
ğŸ“š FONTES CONSULTADAS
============================================================

--- Fonte 1 (relevÃ¢ncia: 0.8542) ---
[Trecho do documento que foi usado como contexto]
...
```

## PrÃ³ximos Passos

ApÃ³s configurar o sistema, vocÃª pode:

- Adicionar mais documentos ao banco vetorial
- Implementar uma API REST para consultas
- Criar interface web com Streamlit ou Gradio
- Adicionar suporte para outros formatos (DOCX, TXT, etc.)
- Implementar cache de respostas para perguntas frequentes
- Adicionar filtros por metadados na busca
- Integrar com sistemas de autenticaÃ§Ã£o

## Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  document.pdfâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ingest.py     â”‚ â”€â”€â”€ Chunking (1000 chars, overlap 150)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Embeddings    â”‚ â”€â”€â”€ OpenAI ou Google
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL +   â”‚ â”€â”€â”€ Armazenamento vetorial
â”‚    pgVector     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    search.py    â”‚ â”€â”€â”€ Busca semÃ¢ntica (k=10)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LLM         â”‚ â”€â”€â”€ OpenAI ou Google
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    chat.py      â”‚ â”€â”€â”€ Interface CLI
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## LicenÃ§a

Este projeto Ã© fornecido como exemplo educacional.